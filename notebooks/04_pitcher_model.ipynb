{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# 04 - Pitcher Model Training\n\nTrain models to predict pitcher fantasy points per IP (skill-based).\n\n**Models:** Random Forest, XGBoost, LightGBM (tree-based only to capture non-linear relationships)\n\n**Validation:** Hold out 2024-2025 data for final evaluation\n\n**Target:** `Fpoints_IP` (fantasy points per inning pitched)\n\n**Note:** This model predicts skill-based points only (3*IP + K - BB - H - 2*ER). W/L/Hold/Save will be added via external projections at prediction time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\n\n# Set working directory to project root\nnotebook_dir = os.path.dirname(os.path.abspath('__file__'))\nif 'notebooks' in os.getcwd():\n    os.chdir('..')\nproject_root = os.getcwd()\nsys.path.insert(0, project_root)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport shap\n\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom config.settings import PROCESSED_DATA_DIR, MODELS_DIR, RANDOM_STATE\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed\nnp.random.seed(RANDOM_STATE)\n\nprint(f\"Project root: {project_root}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "df = pd.read_csv(f\"{PROCESSED_DATA_DIR}/pitchers_processed.csv\")\n",
    "print(f\"Loaded {len(df)} pitcher-seasons\")\n",
    "print(f\"Years: {df['Season'].min()} - {df['Season'].max()}\")\n",
    "print(f\"\\nTarget (Fpoints_IP): mean={df['Fpoints_IP'].mean():.3f}, std={df['Fpoints_IP'].std():.3f}\")\n",
    "print(f\"\\nRole distribution:\")\n",
    "print(f\"  SP (GS > 0): {(df['GS'] > 0).sum()}\")\n",
    "print(f\"  RP (GS = 0): {(df['GS'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (lag and rolling features only)\n",
    "id_cols = ['IDfg', 'Season', 'Name', 'Team']\n",
    "target_col = 'Fpoints_IP'\n",
    "\n",
    "# Model features: lag1, lag2, rolling averages, and arsenal stats\n",
    "feature_cols = [c for c in df.columns if '_lag' in c or '_avg' in c]\n",
    "\n",
    "# Also include arsenal columns if present (fastball velo, spin, etc.)\n",
    "arsenal_cols = [c for c in df.columns if c.startswith(('ff_', 'si_', 'sl_', 'ch_', 'cu_'))]\n",
    "feature_cols = feature_cols + [c for c in arsenal_cols if c not in feature_cols]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"  lag1: {len([c for c in feature_cols if '_lag1' in c])}\")\n",
    "print(f\"  lag2: {len([c for c in feature_cols if '_lag2' in c])}\")\n",
    "print(f\"  rolling avg: {len([c for c in feature_cols if '_avg' in c])}\")\n",
    "print(f\"  arsenal: {len(arsenal_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Train/Validation Split\n",
    "\n",
    "Hold out 2024-2025 for validation (most recent data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: train on 2016-2023, validate on 2024-2025\n",
    "train_years = [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "val_years = [2024, 2025]\n",
    "\n",
    "train_df = df[df['Season'].isin(train_years)].copy()\n",
    "val_df = df[df['Season'].isin(val_years)].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df)} rows ({train_years[0]}-{train_years[-1]})\")\n",
    "print(f\"Validation set: {len(val_df)} rows ({val_years[0]}-{val_years[-1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[target_col].copy()\n",
    "\n",
    "X_val = val_df[feature_cols].copy()\n",
    "y_val = val_df[target_col].copy()\n",
    "\n",
    "# Handle any remaining NaN values (fill with column median from training set)\n",
    "train_medians = X_train.median()\n",
    "X_train = X_train.fillna(train_medians)\n",
    "X_val = X_val.fillna(train_medians)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"\\nNaN counts - train: {X_train.isna().sum().sum()}, val: {X_val.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Model Training & Hyperparameter Optimization\n\nTree-based models only (no linear models - we want to capture non-linear relationships).\n\n### 1. Random Forest"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# XGBoost with randomized search\nxgb_params = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7, 10],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.7, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'gamma': [0, 1, 5]\n}\n\nxgb = XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)\nxgb_cv = RandomizedSearchCV(xgb, xgb_params, n_iter=10, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1, random_state=RANDOM_STATE)\nxgb_cv.fit(X_train, y_train)\n\nprint(f\"\\nBest XGB params: {xgb_cv.best_params_}\")\nprint(f\"CV MAE: {-xgb_cv.best_score_:.4f}\")\n\nxgb_best = xgb_cv.best_estimator_"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "### 3. LightGBM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# LightGBM with randomized search\nlgb_params = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15, -1],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [20, 31, 50],\n    'subsample': [0.7, 0.8, 1.0]\n}\n\nlgb = LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)\nlgb_cv = RandomizedSearchCV(lgb, lgb_params, n_iter=10, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1, random_state=RANDOM_STATE)\nlgb_cv.fit(X_train, y_train)\n\nprint(f\"\\nBest LGB params: {lgb_cv.best_params_}\")\nprint(f\"CV MAE: {-lgb_cv.best_score_:.4f}\")\n\nlgb_best = lgb_cv.best_estimator_"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### 4. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, X, y, model_name):\n    \"\"\"Evaluate model and return metrics.\"\"\"\n    y_pred = model.predict(X)\n    \n    mae = mean_absolute_error(y, y_pred)\n    rmse = np.sqrt(mean_squared_error(y, y_pred))\n    r2 = r2_score(y, y_pred)\n    \n    return {'Model': model_name, 'MAE': mae, 'RMSE': rmse, 'RÂ²': r2}\n\n# Evaluate all models\nresults = []\nresults.append(evaluate_model(rf_best, X_val, y_val, 'Random Forest'))\nresults.append(evaluate_model(xgb_best, X_val, y_val, 'XGBoost'))\nresults.append(evaluate_model(lgb_best, X_val, y_val, 'LightGBM'))\n\nresults_df = pd.DataFrame(results)\nprint(\"\\n=== Validation Results ===\")\nprint(results_df.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "# Select best model based on MAE\nbest_model_name = results_df.loc[results_df['MAE'].idxmin(), 'Model']\nprint(f\"\\nBest model: {best_model_name}\")\n\n# Get the best model object\nmodels = {\n    'Random Forest': rf_best,\n    'XGBoost': xgb_best,\n    'LightGBM': lgb_best\n}\n\nbest_model = models[best_model_name]\nneeds_scaling = False  # Tree models don't need scaling"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, model_name, use_scaled=False):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    if use_scaled:\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X)\n",
    "    \n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    return {'Model': model_name, 'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "results.append(evaluate_model(ridge_best, X_val_scaled, y_val, 'Ridge', use_scaled=True))\n",
    "results.append(evaluate_model(rf_best, X_val, y_val, 'Random Forest'))\n",
    "results.append(evaluate_model(xgb_best, X_val, y_val, 'XGBoost'))\n",
    "results.append(evaluate_model(lgb_best, X_val, y_val, 'LightGBM'))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Validation Results (2024-2025) ===\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Get feature importance from tree-based model\nimportance_df = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': best_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 20 Most Important Features:\")\nprint(importance_df.head(20).to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best tree-based model\n",
    "if best_model_name in ['Random Forest', 'XGBoost', 'LightGBM']:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "else:\n",
    "    # For linear models, use coefficients\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': np.abs(best_model.coef_)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Calculate SHAP values for tree-based model\nexplainer = shap.TreeExplainer(best_model)\nshap_values = explainer(X_val)\n# Ensure feature names are attached\nshap_values.feature_names = feature_cols\nprint(f\"SHAP values calculated for {len(X_val)} validation samples\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Calculate SHAP values for tree-based models\nif best_model_name in ['Random Forest', 'XGBoost', 'LightGBM']:\n    explainer = shap.TreeExplainer(best_model)\n    shap_values = explainer(X_val)\n    # Ensure feature names are attached\n    shap_values.feature_names = feature_cols\n    print(f\"SHAP values calculated for {len(X_val)} validation samples\")\nelse:\n    # For linear models, use Linear explainer\n    explainer = shap.LinearExplainer(best_model, X_val_scaled)\n    shap_values = explainer(X_val_scaled)\n    shap_values.feature_names = feature_cols\n    print(f\"SHAP values calculated for {len(X_val)} validation samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# SHAP summary plot\nplt.figure(figsize=(10, 10))\nshap.summary_plot(shap_values, X_val, feature_names=feature_cols, max_display=20, show=False)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Player Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "def predict_player(player_name, season, show_shap=True):\n    \"\"\"\n    Predict fantasy points for a specific pitcher and season.\n    Shows SHAP waterfall plot for explainability.\n    \n    Note: This predicts skill-based Fpoints/IP only.\n    W/L/Hold/Save must be added separately from external projections.\n    \n    Args:\n        player_name: Player name (partial match supported)\n        season: Season year (e.g., 2024)\n        show_shap: Whether to display SHAP waterfall plot\n    \n    Returns:\n        dict with prediction details\n    \"\"\"\n    # Find player\n    mask = df['Name'].str.contains(player_name, case=False) & (df['Season'] == season)\n    player_df = df[mask]\n    \n    if len(player_df) == 0:\n        print(f\"Pitcher '{player_name}' not found in {season}\")\n        # Show similar names\n        similar = df[df['Name'].str.contains(player_name, case=False)]['Name'].unique()\n        if len(similar) > 0:\n            print(f\"Similar names: {similar[:5]}\")\n        return None\n    \n    if len(player_df) > 1:\n        print(f\"Multiple matches found: {player_df['Name'].values}\")\n        player_df = player_df.iloc[[0]]\n    \n    player_row = player_df.iloc[0]\n    X_player = player_df[feature_cols].fillna(train_medians)\n    \n    # Predict\n    if needs_scaling:\n        X_player_scaled = scaler.transform(X_player)\n        pred = best_model.predict(X_player_scaled)[0]\n    else:\n        pred = best_model.predict(X_player)[0]\n    \n    actual = player_row['Fpoints_IP']\n    error = pred - actual\n    \n    # Determine role\n    role = \"SP\" if player_row['GS'] > 0 else \"RP\"\n    \n    print(f\"\\n=== {player_row['Name']} ({season}) ===\")\n    print(f\"Team: {player_row['Team']} | Role: {role}\")\n    print(f\"IP: {player_row['IP']:.1f} | G: {player_row['G']:.0f} | GS: {player_row['GS']:.0f}\")\n    print(f\"\\nActual Fpoints/IP (skill): {actual:.3f}\")\n    print(f\"Predicted Fpoints/IP (skill): {pred:.3f}\")\n    print(f\"Error: {error:+.3f}\")\n    \n    # Project total skill-based points (using actual IP)\n    total_actual = actual * player_row['IP']\n    total_pred = pred * player_row['IP']\n    print(f\"\\nTotal Skill Fpoints (actual IP):\")\n    print(f\"  Actual: {total_actual:.0f}\")\n    print(f\"  Predicted: {total_pred:.0f}\")\n    \n    # Show actual W/L/S/HLD for context\n    if 'W' in player_row and 'L' in player_row:\n        print(f\"\\nActual W/L/SV/HLD: {player_row.get('W', 0):.0f}-{player_row.get('L', 0):.0f}, {player_row.get('SV', 0):.0f} SV, {player_row.get('HLD', 0):.0f} HLD\")\n        # Calculate team-based points\n        team_pts = 2*player_row.get('W', 0) - 2*player_row.get('L', 0) + 5*player_row.get('SV', 0) + 2*player_row.get('HLD', 0)\n        print(f\"  Team-based points: {team_pts:.0f}\")\n        print(f\"  Total actual (skill + team): {total_actual + team_pts:.0f}\")\n    \n    # SHAP waterfall\n    if show_shap:\n        if needs_scaling:\n            player_shap = explainer(X_player_scaled)\n        else:\n            player_shap = explainer(X_player)\n        \n        # Attach feature names and UNSCALED data for interpretability\n        player_shap.feature_names = feature_cols\n        player_shap.data = X_player.values  # Use unscaled values for display\n        \n        plt.figure(figsize=(10, 8))\n        shap.plots.waterfall(player_shap[0], max_display=15, show=False)\n        plt.title(f\"SHAP Waterfall: {player_row['Name']} ({season})\")\n        plt.tight_layout()\n        plt.show()\n    \n    return {\n        'name': player_row['Name'],\n        'season': season,\n        'role': role,\n        'actual': actual,\n        'predicted': pred,\n        'error': error,\n        'ip': player_row['IP']\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: predict for a specific pitcher (SP)\n",
    "predict_player(\"Gerrit Cole\", 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a reliever\n",
    "predict_player(\"Emmanuel Clase\", 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 2025 Predictions & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_season(season, role_filter=None):\n",
    "    \"\"\"\n",
    "    Predict for all pitchers in a given season and compare to actuals.\n",
    "    \n",
    "    Args:\n",
    "        season: Year to predict\n",
    "        role_filter: 'SP', 'RP', or None for all\n",
    "    \n",
    "    Returns DataFrame with predictions and actuals.\n",
    "    \"\"\"\n",
    "    season_df = df[df['Season'] == season].copy()\n",
    "    \n",
    "    if role_filter == 'SP':\n",
    "        season_df = season_df[season_df['GS'] > 0]\n",
    "    elif role_filter == 'RP':\n",
    "        season_df = season_df[season_df['GS'] == 0]\n",
    "    \n",
    "    if len(season_df) == 0:\n",
    "        print(f\"No data for {season}\")\n",
    "        return None\n",
    "    \n",
    "    X_season = season_df[feature_cols].fillna(train_medians)\n",
    "    \n",
    "    if needs_scaling:\n",
    "        X_season_scaled = scaler.transform(X_season)\n",
    "        preds = best_model.predict(X_season_scaled)\n",
    "    else:\n",
    "        preds = best_model.predict(X_season)\n",
    "    \n",
    "    results = season_df[['Name', 'Team', 'IP', 'GS', 'Fpoints_IP']].copy()\n",
    "    results['Role'] = results['GS'].apply(lambda x: 'SP' if x > 0 else 'RP')\n",
    "    results['Predicted'] = preds\n",
    "    results['Error'] = results['Predicted'] - results['Fpoints_IP']\n",
    "    results['Abs_Error'] = results['Error'].abs()\n",
    "    \n",
    "    # Calculate total points (skill-based only)\n",
    "    results['Actual_Total'] = results['Fpoints_IP'] * results['IP']\n",
    "    results['Predicted_Total'] = results['Predicted'] * results['IP']\n",
    "    \n",
    "    # Sort by predicted total\n",
    "    results = results.sort_values('Predicted_Total', ascending=False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def show_season_summary(season, role_filter=None):\n",
    "    \"\"\"\n",
    "    Show summary of predictions for a season.\n",
    "    \n",
    "    Args:\n",
    "        season: Year to show\n",
    "        role_filter: 'SP', 'RP', or None for all\n",
    "    \"\"\"\n",
    "    results = predict_season(season, role_filter)\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    role_str = f\" ({role_filter})\" if role_filter else \"\"\n",
    "    print(f\"\\n=== {season} Prediction Summary{role_str} ===\")\n",
    "    print(f\"Pitchers: {len(results)}\")\n",
    "    print(f\"MAE: {results['Abs_Error'].mean():.4f}\")\n",
    "    print(f\"RMSE: {np.sqrt((results['Error']**2).mean()):.4f}\")\n",
    "    \n",
    "    # Correlation between predicted and actual rankings\n",
    "    results['Actual_Rank'] = results['Actual_Total'].rank(ascending=False)\n",
    "    results['Predicted_Rank'] = results['Predicted_Total'].rank(ascending=False)\n",
    "    rank_corr = results['Actual_Rank'].corr(results['Predicted_Rank'])\n",
    "    print(f\"Rank Correlation: {rank_corr:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop 15 Predicted (skill-based Fpoints):\")\n",
    "    display_cols = ['Name', 'Team', 'Role', 'IP', 'Fpoints_IP', 'Predicted', 'Actual_Total', 'Predicted_Total']\n",
    "    print(results[display_cols].head(15).to_string(index=False))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 2025 predictions (all pitchers)\n",
    "results_2025 = show_season_summary(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 2025 SP only\n",
    "results_2025_sp = show_season_summary(2025, role_filter='SP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 2025 RP only\n",
    "results_2025_rp = show_season_summary(2025, role_filter='RP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: predicted vs actual\n",
    "if results_2025 is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # All pitchers\n",
    "    ax = axes[0]\n",
    "    colors = results_2025['Role'].map({'SP': 'blue', 'RP': 'orange'})\n",
    "    ax.scatter(results_2025['Fpoints_IP'], results_2025['Predicted'], c=colors, alpha=0.5)\n",
    "    min_val = min(results_2025['Fpoints_IP'].min(), results_2025['Predicted'].min())\n",
    "    max_val = max(results_2025['Fpoints_IP'].max(), results_2025['Predicted'].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "    ax.set_xlabel('Actual Fpoints/IP')\n",
    "    ax.set_ylabel('Predicted Fpoints/IP')\n",
    "    ax.set_title('2025 Predictions vs Actuals (All)')\n",
    "    ax.legend(['Perfect', 'SP', 'RP'])\n",
    "    \n",
    "    # By role\n",
    "    ax = axes[1]\n",
    "    for role, color in [('SP', 'blue'), ('RP', 'orange')]:\n",
    "        role_df = results_2025[results_2025['Role'] == role]\n",
    "        ax.scatter(role_df['Fpoints_IP'], role_df['Predicted'], c=color, alpha=0.5, label=role)\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    ax.set_xlabel('Actual Fpoints/IP')\n",
    "    ax.set_ylabel('Predicted Fpoints/IP')\n",
    "    ax.set_title('2025 Predictions by Role')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 2024 predictions as well\n",
    "results_2024 = show_season_summary(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## SP vs RP Model Performance\n",
    "\n",
    "Check if performance differs significantly between starters and relievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SP vs RP performance\n",
    "if results_2025 is not None:\n",
    "    print(\"\\n=== SP vs RP Model Performance (2025) ===\")\n",
    "    for role in ['SP', 'RP']:\n",
    "        role_df = results_2025[results_2025['Role'] == role]\n",
    "        mae = role_df['Abs_Error'].mean()\n",
    "        rmse = np.sqrt((role_df['Error']**2).mean())\n",
    "        print(f\"{role}: MAE={mae:.4f}, RMSE={rmse:.4f}, n={len(role_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Save model and metadata\n",
    "model_data = {\n",
    "    'model': best_model,\n",
    "    'model_name': best_model_name,\n",
    "    'scaler': scaler if needs_scaling else None,\n",
    "    'needs_scaling': needs_scaling,\n",
    "    'feature_cols': feature_cols,\n",
    "    'train_medians': train_medians,\n",
    "    'train_years': train_years,\n",
    "    'val_results': results_df.to_dict(),\n",
    "    'note': 'Predicts skill-based Fpoints/IP only. Add W/L/Hold/Save from external projections.'\n",
    "}\n",
    "\n",
    "model_path = f\"{MODELS_DIR}/pitcher_model.joblib\"\n",
    "joblib.dump(model_data, model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## Interactive Player Lookup\n",
    "\n",
    "Use the cells below to look up any pitcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change player name and year as needed\n",
    "predict_player(\"Zack Wheeler\", 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available pitchers for a year\n",
    "year = 2025\n",
    "pitchers = df[df['Season'] == year].copy()\n",
    "pitchers['Role'] = pitchers['GS'].apply(lambda x: 'SP' if x > 0 else 'RP')\n",
    "pitchers = pitchers.sort_values('Fpoints_IP', ascending=False)\n",
    "\n",
    "print(f\"Top 20 pitchers by Fpoints/IP in {year}:\")\n",
    "print(pitchers[['Name', 'Team', 'Role', 'IP', 'Fpoints_IP']].head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List top SPs\n",
    "print(f\"\\nTop 15 SPs by Fpoints/IP in {year}:\")\n",
    "sps = pitchers[pitchers['Role'] == 'SP']\n",
    "print(sps[['Name', 'Team', 'IP', 'Fpoints_IP']].head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List top RPs\n",
    "print(f\"\\nTop 15 RPs by Fpoints/IP in {year}:\")\n",
    "rps = pitchers[pitchers['Role'] == 'RP']\n",
    "print(rps[['Name', 'Team', 'IP', 'Fpoints_IP']].head(15).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}